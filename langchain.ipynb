{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (ChatPromptTemplate, HumanMessagePromptTemplate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(template=\"Can i answer this or do i need a tool? {query}\", input_variables=[\"query\"]))\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, let me answer that the current president of America is Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(chain.run(\"who is the current president of america\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents import (initialize_agent, AgentType)\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "\n",
    "general_query_prompt = PromptTemplate(template=\"{query}\",input_variables=[\"query\"])\n",
    "\n",
    "general_query = LLMChain(llm=llm, prompt=general_query_prompt)\n",
    "\n",
    "general_query_tool = Tool(\n",
    "    name=\"Language Model\",\n",
    "    func=general_query.run,\n",
    "    description=\"use this tool for general conversation and logic, not for current information\"\n",
    ")\n",
    "\n",
    "\n",
    "tools = load_tools(['serpapi'])\n",
    "tools[0].name=\"Intermediate Answer\"\n",
    "tools[0].description=\"google search\"\n",
    "# tools.append(general_query_tool)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exactly one tool must be specified, but got [Tool(name='Intermediate Answer', description='google search', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x0000020358B162C0>, func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='f322a794ed8efa037d4ad79467a1814d87d22adc19aea570bdb2b3d7b6c17870', aiosession=None)>, coroutine=None), Tool(name='Language Model', description='use this tool for general conversation and logic, not for current information', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x0000020358B162C0>, func=<bound method Chain.run of LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x0000020358B162C0>, verbose=False, prompt=PromptTemplate(input_variables=['query'], output_parser=None, partial_variables={}, template='{query}', template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x0000020358B162C0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False), output_key='text')>, coroutine=None)]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 31\u001B[0m\n\u001B[0;32m     29\u001B[0m tools\u001B[38;5;241m.\u001B[39mappend(general_query_tool)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# initialize the search enabled agent\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m self_ask_with_search \u001B[38;5;241m=\u001B[39m \u001B[43minitialize_agent\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mself-ask-with-search\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iteration\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\n\u001B[0;32m     37\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\label-studio\\lib\\site-packages\\langchain\\agents\\initialize.py:52\u001B[0m, in \u001B[0;36minitialize_agent\u001B[1;34m(tools, llm, agent, callback_manager, agent_path, agent_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m     50\u001B[0m     agent_cls \u001B[38;5;241m=\u001B[39m AGENT_TO_CLASS[agent]\n\u001B[0;32m     51\u001B[0m     agent_kwargs \u001B[38;5;241m=\u001B[39m agent_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m---> 52\u001B[0m     agent_obj \u001B[38;5;241m=\u001B[39m agent_cls\u001B[38;5;241m.\u001B[39mfrom_llm_and_tools(\n\u001B[0;32m     53\u001B[0m         llm, tools, callback_manager\u001B[38;5;241m=\u001B[39mcallback_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39magent_kwargs\n\u001B[0;32m     54\u001B[0m     )\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m agent_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     56\u001B[0m     agent_obj \u001B[38;5;241m=\u001B[39m load_agent(\n\u001B[0;32m     57\u001B[0m         agent_path, llm\u001B[38;5;241m=\u001B[39mllm, tools\u001B[38;5;241m=\u001B[39mtools, callback_manager\u001B[38;5;241m=\u001B[39mcallback_manager\n\u001B[0;32m     58\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\label-studio\\lib\\site-packages\\langchain\\agents\\agent.py:506\u001B[0m, in \u001B[0;36mAgent.from_llm_and_tools\u001B[1;34m(cls, llm, tools, callback_manager, **kwargs)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    498\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_llm_and_tools\u001B[39m(\n\u001B[0;32m    499\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    503\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    504\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Agent:\n\u001B[0;32m    505\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Construct an agent from an LLM and tools.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 506\u001B[0m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_tools\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtools\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    507\u001B[0m     llm_chain \u001B[38;5;241m=\u001B[39m LLMChain(\n\u001B[0;32m    508\u001B[0m         llm\u001B[38;5;241m=\u001B[39mllm,\n\u001B[0;32m    509\u001B[0m         prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_prompt(tools),\n\u001B[0;32m    510\u001B[0m         callback_manager\u001B[38;5;241m=\u001B[39mcallback_manager,\n\u001B[0;32m    511\u001B[0m     )\n\u001B[0;32m    512\u001B[0m     tool_names \u001B[38;5;241m=\u001B[39m [tool\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m tool \u001B[38;5;129;01min\u001B[39;00m tools]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\label-studio\\lib\\site-packages\\langchain\\agents\\self_ask_with_search\\base.py:31\u001B[0m, in \u001B[0;36mSelfAskWithSearchAgent._validate_tools\u001B[1;34m(cls, tools)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_tools\u001B[39m(\u001B[38;5;28mcls\u001B[39m, tools: Sequence[BaseTool]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(tools) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 31\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExactly one tool must be specified, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtools\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     32\u001B[0m     tool_names \u001B[38;5;241m=\u001B[39m {tool\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m tool \u001B[38;5;129;01min\u001B[39;00m tools}\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tool_names \u001B[38;5;241m!=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIntermediate Answer\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n",
      "\u001B[1;31mValueError\u001B[0m: Exactly one tool must be specified, but got [Tool(name='Intermediate Answer', description='google search', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x0000020358B162C0>, func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='f322a794ed8efa037d4ad79467a1814d87d22adc19aea570bdb2b3d7b6c17870', aiosession=None)>, coroutine=None), Tool(name='Language Model', description='use this tool for general conversation and logic, not for current information', return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x0000020358B162C0>, func=<bound method Chain.run of LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x0000020358B162C0>, verbose=False, prompt=PromptTemplate(input_variables=['query'], output_parser=None, partial_variables={}, template='{query}', template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x0000020358B162C0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False), output_key='text')>, coroutine=None)]"
     ]
    }
   ],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "from langchain import OpenAI\n",
    "llm = OpenAI(\n",
    "    openai_api_key=\"___\",\n",
    "    temperature=0,\n",
    "    model_name=\"text-davinci-003\"\n",
    ")\n",
    "# initialize the search chain\n",
    "search = SerpAPIWrapper(serpapi_api_key='___')\n",
    "\n",
    "# create a search tool\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description='google search'\n",
    "    )\n",
    "]\n",
    "\n",
    "general_query_prompt = PromptTemplate(template=\"{query}\",input_variables=[\"query\"])\n",
    "\n",
    "general_query = LLMChain(llm=llm, prompt=general_query_prompt)\n",
    "\n",
    "general_query_tool = Tool(\n",
    "    name=\"Language Model\",\n",
    "    func=general_query.run,\n",
    "    description=\"use this tool for general conversation and logic, not for current information\"\n",
    ")\n",
    "tools.append(general_query_tool)\n",
    "# initialize the search enabled agent\n",
    "self_ask_with_search = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=\"self-ask-with-search\",\n",
    "    verbose=True,\n",
    "    max_iteration=3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "\n",
    "agent = initialize_agent(\n",
    "    agent=AgentType.SELF_ASK_WITH_SEARCH,\n",
    "    tools = tools,\n",
    "    llm = llm,\n",
    "    verbose=True,\n",
    "    max_iteration=3,\n",
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m No.\n",
      "So the final answer is: No follow up questions are needed.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'No follow up questions are needed.'"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_ask_with_search.run(\"hi\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(self_ask_with_search.agent.llm_chain.prompt.template)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mThought: Do I need to use a tool? No\n",
      "AI: To push a Git commit from your local repository to a remote repository, you can use the following command:\n",
      "\n",
      "```\n",
      "git push <remote> <branch>\n",
      "```\n",
      "\n",
      "Here, `<remote>` is the name of the remote repository you want to push to (such as `origin`), and `<branch>` is the name of the branch you want to push (such as `master`). So if you want to push your local changes to the `master` branch of the `origin` remote, you would run the following command:\n",
      "\n",
      "```\n",
      "git push origin master\n",
      "```\n",
      "\n",
      "This will push your changes to the remote repository and make them available to others who have access to that repository.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'To push a Git commit from your local repository to a remote repository, you can use the following command:\\n\\n```\\ngit push <remote> <branch>\\n```\\n\\nHere, `<remote>` is the name of the remote repository you want to push to (such as `origin`), and `<branch>` is the name of the branch you want to push (such as `master`). So if you want to push your local changes to the `master` branch of the `origin` remote, you would run the following command:\\n\\n```\\ngit push origin master\\n```\\n\\nThis will push your changes to the remote repository and make them available to others who have access to that repository.'"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"give me code to push git commit from local to remote \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Search\n",
      "Action Input: who is the current president of india and their age\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3m64 years\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mDo I need to use a tool? Yes\n",
      "Action: Language Model\n",
      "Action Input: Could you please clarify your previous input? I'm not sure what you mean by \"64 years\".\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mI apologize for any confusion. As an AI language model, I do not have any previous input as I generate responses based on the input given to me. Could you kindly provide me with more context or information so that I may better understand the question or statement you are referring to?\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mDo I need to use a tool? No\n",
      "AI: My previous response was a bit confusing, as I am not sure what \"64 years\" refers to. To answer your initial question, the current President of India is Ram Nath Kovind. However, I do not have his exact age on hand.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'My previous response was a bit confusing, as I am not sure what \"64 years\" refers to. To answer your initial question, the current President of India is Ram Nath Kovind. However, I do not have his exact age on hand.'"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"about president of india\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
